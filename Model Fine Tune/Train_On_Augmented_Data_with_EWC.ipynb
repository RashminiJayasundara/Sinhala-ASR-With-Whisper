{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st3E1Y98Ni1t"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "hLAA0pjhNqSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "MCiLvtxwNqVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Dfu_DUQMNqY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y0EbzT_zNqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Sinhala\", task=\"transcribe\")\n"
      ],
      "metadata": {
        "id": "_jxEmKNCNqfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Sinhala\", task=\"transcribe\")\n"
      ],
      "metadata": {
        "id": "UhOULbiFNqiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk, Dataset\n",
        "save_path = '/content/drive/My Drive/asr_sinhala/ProcessedData/FMI_dataset'\n",
        "old_dataset = load_from_disk(save_path)"
      ],
      "metadata": {
        "id": "ZvSXp9KzNqlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk, Dataset\n",
        "save_path = '/content/drive/My Drive/asr_sinhala/ProcessedData/trained_dataset_d'\n",
        "new_dataset = load_from_disk(save_path)"
      ],
      "metadata": {
        "id": "pz4FJF3qN4Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "      input_features = []\n",
        "      for feature in features:\n",
        "          if \"input_features\" not in feature:\n",
        "              print(\"Warning: 'input_features' not found in feature:\", feature)\n",
        "              continue\n",
        "          input_features.append({\"input_features\": feature[\"input_features\"]})\n",
        "\n",
        "\n",
        "      batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "      label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features if \"labels\" in feature]\n",
        "      if len(label_features) == 0:\n",
        "          raise ValueError(\"No valid 'labels' found in the features.\")\n",
        "\n",
        "      labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "      labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "      if labels.size(1) > 0 and (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "          labels = labels[:, 1:]\n",
        "\n",
        "      batch[\"labels\"] = labels\n",
        "\n",
        "      return batch"
      ],
      "metadata": {
        "id": "-pmKdkxRN4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"RRashmini/whisper-small-sinhala-1\").to(device)"
      ],
      "metadata": {
        "id": "SVzDuedPN4S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_fisher_information(model,dataloader,device):\n",
        "  fisher_info = {name: torch.zeros_like(param,device=device) for name,param in model.named_parameters()}\n",
        "  model.eval()\n",
        "  for batch in dataloader :\n",
        "    inputs= batch['input_features'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_features=inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    for name, param in model.named_parameters():\n",
        "\n",
        "      fisher_info[name] += param.grad.data.pow(2)\n",
        "  for name in fisher_info:\n",
        "    fisher_info[name] /= len(dataloader)\n",
        "  return fisher_info"
      ],
      "metadata": {
        "id": "5Nd2vVrDOZTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ewc_loss(model,fisher_info,prev_params,lambda_ewc=10):\n",
        "  loss =0\n",
        "  for name, param in model.named_parameters():\n",
        "    loss += (fisher_info[name]* (param-prev_params[name])**2).sum()\n",
        "  return lambda_ewc*loss"
      ],
      "metadata": {
        "id": "UjK7jE_9OZXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "class EWCTrainer(Seq2SeqTrainer):\n",
        "  def __init__(self, model, fisher_info, prev_params, lambda_ewc=10, *args, **kwargs):\n",
        "    super().__init__(model, *args, **kwargs)\n",
        "    self.fisher_info = fisher_info\n",
        "    self.prev_params = prev_params\n",
        "    self.lambda_ewc = lambda_ewc\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False,*args, **kwargs):\n",
        "    outputs = model(**inputs)\n",
        "    loss = outputs.loss\n",
        "    ewc_reg = ewc_loss(model,self.fisher_info,self.prev_params,self.lambda_ewc)\n",
        "    loss += ewc_reg\n",
        "\n",
        "    return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "BMKEbKmrPGp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev_params = {name: param.clone().detach() for name, param in model.named_parameters()}"
      ],
      "metadata": {
        "id": "AhhI9l4IPGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "0ozHG7BFPGwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "wGYqR4nxPUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "fisher_dataloader = DataLoader(\n",
        "    old_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle = True,\n",
        "    collate_fn = data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "ZjqUK-mCPUpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_info = compute_fisher_information(model, fisher_dataloader ,device)"
      ],
      "metadata": {
        "id": "sp4w_JduPZqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_save_path = \"/content/drive/MyDrive/asr_sinhala/fisher_info.pt\"\n",
        "def save_fisher_info(load_path):\n",
        "    torch.save(fisher_info, load_path)\n",
        "    print(f\"✅ Fisher Information saved to {load_path}\")\n",
        "\n",
        "save_fisher_info(fisher_save_path)"
      ],
      "metadata": {
        "id": "QfBDrRuMPZt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_save_path = \"/content/drive/MyDrive/asr_sinhala/fisher_info.pt\"\n",
        "def load_fisher_info(load_path):\n",
        "    fisher_info = torch.load(load_path)\n",
        "    print(f\"✅ Fisher Information loaded from {load_path}\")\n",
        "    return fisher_info\n",
        "\n",
        "fisher_info = load_fisher_info(fisher_save_path)"
      ],
      "metadata": {
        "id": "rI8ynYpHPZxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-specaugment-sinhala\",\n",
        "    logging_steps=100,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "2h7m8jiDPUtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "YnKwIUVGPrWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "dskSRmQkPsXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate"
      ],
      "metadata": {
        "id": "TiTW_yB6PsbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk, Dataset\n",
        "save_path_test = '/content/drive/My Drive/asr_sinhala/test_dataset_s'\n",
        "processed_test_dataset = load_from_disk(save_path_test)"
      ],
      "metadata": {
        "id": "DkNB3TBCPxDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "gFvNdHAaPxGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "LcYoDULePxJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = EWCTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    train_dataset=new_dataset,\n",
        "    eval_dataset= processed_test_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    fisher_info=fisher_info,\n",
        "    prev_params=prev_params,\n",
        "    lambda_ewc=10,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "_c8-ovKbQLAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2jJ6iL53QLMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=processed_test_dataset)"
      ],
      "metadata": {
        "id": "gV-7Cu9TQRQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"RRashmini/whisper-small-sinhala-2\")"
      ],
      "metadata": {
        "id": "Y0rrF8IAQRUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}